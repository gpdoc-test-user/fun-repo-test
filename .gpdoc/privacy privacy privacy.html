<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>privacy privacy privacy</title>
  <meta name="generator" content="GPDoc (Embedded)">
  
  <!-- Preload critical resources -->
  <link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" as="style">
  <link rel="preload" href="https://cdn.jsdelivr.net/npm/marked/marked.min.js" as="script">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11/styles/github.min.css">
  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/highlight.js@11/lib/highlight.min.js"></script>
  <style>
    :root {
      --bg-color: #FFFFFF;
      --surface-color: #F8FAFC;
      --primary-text: #111827;
      --secondary-text: #4B5563;
      --accent-color: #2563EB;
      --border-color: #E2E8F0;
      --code-bg: #0F172A;
      --code-text: #F8FAFC;
      --quote-border: #2563EB;
    }
    
    * { box-sizing: border-box; }
    
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      max-width: 800px;
      margin: 0 auto;
      padding: 40px 20px;
      line-height: 1.6;
      font-size: 16px;
      background-color: var(--bg-color);
      color: var(--primary-text);
    }
    
    h1, h2, h3, h4, h5, h6 {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      font-weight: 600;
      color: var(--primary-text);
      margin-top: 1.5em;
      margin-bottom: 0.5em;
      line-height: 1.3;
    }
    
    h1 { font-size: 2.25em; margin-top: 0; }
    h2 { font-size: 1.75em; }
    h3 { font-size: 1.5em; }
    h4 { font-size: 1.25em; }
    
    p { margin: 1em 0; }
    
    a { color: var(--accent-color); text-decoration: none; }
    a:hover { text-decoration: underline; }
    
    pre {
      background: var(--code-bg);
      color: var(--code-text);
      padding: 16px 20px;
      border-radius: 8px;
      overflow-x: auto;
      font-family: monospace, 'Consolas', 'Monaco', monospace;
      font-size: 0.9em;
      line-height: 1.5;
      margin: 1.5em 0;
    }
    
    code {
      font-family: monospace, 'Consolas', 'Monaco', monospace;
      background: var(--surface-color);
      padding: 2px 6px;
      border-radius: 4px;
      font-size: 0.9em;
    }
    
    pre code {
      background: none;
      padding: 0;
      color: inherit;
      font-size: inherit;
    }
    
    img { max-width: 100%; height: auto; border-radius: 8px; }
    
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 1.5em 0;
    }
    
    th, td {
      border: 1px solid var(--border-color);
      padding: 10px 14px;
      text-align: left;
    }
    
    th {
      background: var(--surface-color);
      font-weight: 600;
    }
    
    tr:nth-child(even) {
      background: var(--surface-color);
    }
    
    blockquote {
      border-left: 4px solid var(--quote-border);
      margin: 1.5em 0;
      padding: 12px 20px;
      background: var(--surface-color);
      color: var(--secondary-text);
      font-style: italic;
    }
    
    blockquote p { margin: 0; }
    blockquote p + p { margin-top: 0.5em; }
    
    ul, ol { padding-left: 1.5em; margin: 1em 0; }
    li { margin: 0.25em 0; }
    li::marker { color: var(--accent-color); }
    
    hr {
      border: none;
      border-top: 1px solid var(--border-color);
      margin: 2em 0;
    }
    
    .mermaid {
      background: var(--surface-color);
      padding: 20px;
      border-radius: 8px;
      text-align: center;
      margin: 1.5em 0;
    }
    
    .katex-display {
      overflow-x: auto;
      overflow-y: hidden;
      padding: 0.5em 0;
    }
    
    @media print {
      body { max-width: none; padding: 20px; }
      pre { white-space: pre-wrap; word-wrap: break-word; }
      .mermaid { break-inside: avoid; }
    }
    
    @media (max-width: 768px) {
      body { padding: 20px 16px; font-size: 15px; }
      h1 { font-size: 1.75em; }
      h2 { font-size: 1.5em; }
      pre { padding: 12px; }
    }
  </style>
</head>
<body>
  <article id="content"></article>
  <script>
    // Initialize mermaid
    mermaid.initialize({
      startOnLoad: false,
      theme: 'default',
      securityLevel: 'loose'
    });
    
    // Configure marked
    const renderer = new marked.Renderer();
    
    renderer.code = function(codeObj, language, escaped) {
      let codeText, lang;
      if (typeof codeObj === 'object' && codeObj !== null) {
        codeText = codeObj.text || codeObj.code || '';
        lang = codeObj.lang || codeObj.language || language || '';
      } else {
        codeText = String(codeObj || '');
        lang = language || '';
      }
      
      if (lang === 'mermaid') {
        const id = 'mermaid-' + Math.random().toString(36).substr(2, 9);
        return '<div class="mermaid" id="' + id + '">' + codeText + '</div>';
      }
      
      if (window.hljs && lang && hljs.getLanguage(lang)) {
        try {
          return '<pre><code class="hljs language-' + lang + '">' + 
            hljs.highlight(codeText, { language: lang }).value + '</code></pre>';
        } catch (e) {}
      }
      
      const escaped_code = String(codeText)
        .replace(/&/g, '&amp;')
        .replace(/</g, '&lt;')
        .replace(/>/g, '&gt;');
      return '<pre><code>' + escaped_code + '</code></pre>';
    };
    
    marked.setOptions({ renderer: renderer, breaks: true, gfm: true });
    
    // Process content
    let processedContent = `# The P Principle — *Privacy–Prediction-Performance*

**Full label:** **P3 Optimal-Intersection Principle (P3-OIP)**

> **One-liner:** Under lawful anonymization ( \$\\varepsilon\$ -DP), there exists an **overlap level** of cross-business datasets at which the **prediction utility gain** from combining data **exceeds** the **performance cost** introduced by privacy noise — yielding a model that strictly outperforms any single-dataset model.

---

# Formal setup

We model (i) legality via DP under legally defined privacy restrictions (e.g., FCRA, GDPR, CCPA); (ii) performance via a risk/utility objective; and (iii) prediction via sample- and correlation-aware variance.

## Entities and notation

* Business datasets: \$\\mathcal{D}_1,\\dots,\\mathcal{D}_D\$.
* Overlap fraction (customer intersection): \$\\alpha \\in [0,1]\$.
* Union size at overlap \$\\alpha\$: \$n(\\alpha)\$ (unique customers).
* Cross-dataset residual correlation as a function of overlap: \$\\rho(\\alpha)\\in[0,1)\$, nondecreasing in \$\\alpha\$.
* Privacy mechanism: \$\\mathsf{M}_\\varepsilon\$ (\$\\varepsilon\$-differentially private), applied to the joint data before training.
* Learning algorithm: ERM (Empirical Risk Minimization) or regularized ERM (ERM with a complexity penalty to discourage overfitting) produces hypothesis \$h \\in \\mathcal{H}\$.
* Population risk: \$\\mathcal{R}(h) = \\mathbb{E}[\\ell(h(X),Y)]\$.
* Utility (higher is better): \$U(h) = -\\mathcal{R}(h)\$ (or any monotone surrogate like AUC-linked regret).

We write:

* **Baseline single-unit** model on \$\\mathcal{D}_1\$: \$h_{\\mathrm{base}}\$ with utility \$U_{\\mathrm{base}}\$.
* **Joint DP model** on \$\\bigcup_{i=1}^D \\mathcal{D}_i\$ at overlap \$\\alpha\$: \$h_{\\mathrm{dp}}(\\alpha,\\varepsilon)\$ with utility \$U_{\\mathrm{dp}}(\\alpha,\\varepsilon)\$.

---

# Legal permissibility as first-order rules (FCRA example)

Let predicates:

These rules are stated generically; FCRA terms are used as an example. Substitute analogous constructs for GDPR and CCPA when mapping to those legal regimes.

* \$\\mathrm{Identifiable}(x)\$ — record contains PII.
* \$\\mathrm{DP}_\\varepsilon(x)\$ — record (or sufficient statistics) produced by an \$\\varepsilon\$-DP mechanism.
* \$\\mathrm{ConsumerReport}(x)\$ — data qualifies as a consumer report.
* \$\\mathrm{PermissibleUse}(x)\$ — FCRA-permissible for cross-unit modelling.

Axioms (policy intent, high-level):

1. \$\\forall x, \\big(\\neg \\mathrm{Identifiable}(x) \\rightarrow \\neg \\mathrm{ConsumerReport}(x)\\big)\$.
2. \$\\forall x, \\big(\\mathrm{DP}_\\varepsilon(x) \\rightarrow \\neg \\mathrm{Identifiable}(x)\\big)\$.
3. \$\\forall x, \\big(\\neg \\mathrm{ConsumerReport}(x) \\rightarrow \\mathrm{PermissibleUse}(x)\\big)\$.

**Lemma (Lawful post-processing).**
If the jointed data are produced by \$\\mathsf{M}_\\varepsilon\$ (DP), then they are non-identifiable and thus permissible for cross-business modelling:

\$\$
\\mathrm{DP}_\\varepsilon(x) \\Rightarrow \\mathrm{PermissibleUse}(x).
\$\$

*(This encodes the DP "post-processing" intuition: lawful anonymization → lawful analytics.)*

---

# Performance model (variance, bias, and DP)

## 1) Variance under cross-dataset ensembling

Suppose each unit induces a model \$h_i\$ with prediction variance \$\\sigma^2\$, and pairwise correlation \$\\rho(\\alpha)\$. Averaging (or stacking with near-uniform weights) over \$D\$ units yields

\$\$
\\mathrm{Var}\\left(\\tfrac{1}{D}\\sum_{i=1}^{D} h_i\\right)
= \\sigma^2\\left(\\rho(\\alpha) + \\frac{1-\\rho(\\alpha)}{D}\\right).
\\tag{V}
\$\$
This captures the **prediction-stability** gain when combining weakly correlated business views.

## 2) Sample size and effective independence

Let \$n(\\alpha)\$ be unique customers. A simple **effective sample size** (ESS) that accounts for correlation is

\$\$
n_{\\mathrm{eff}}(\\alpha) \\approx \\frac{n(\\alpha)}{1+(D-1)\\rho(\\alpha)}.
\\tag{ESS}
\$\$

## 3) DP penalty on excess risk

For ERM with an \$\\varepsilon\$-DP mechanism (e.g., objective perturbation or DP-SGD), the **excess risk** admits bounds of the form

\$\$
\\mathbb{E}\\big[\\mathcal{R}(h_{\\mathrm{dp}}) - \\mathcal{R}(h^\\star)\\big]
\\le \\frac{C(\\mathcal{H},\\delta)}{n_{\\mathrm{eff}}(\\alpha)\\varepsilon^p},
\\qquad p\\in\\{1,2\\},
\\tag{DP}
\$\$

where \$p\$ depends on the mechanism/analysis and \$C(\\cdot)\$ hides dimension, clipping, and confidence terms.
We summarize the **privacy cost** as

\$\$
\\mathrm{Penalty}_{\\mathrm{DP}}(\\alpha,\\varepsilon)
= \\frac{K}{n_{\\mathrm{eff}}(\\alpha)\\varepsilon^p}.
\\tag{P}
\$\$

## 4) Net utility decomposition

Let the **aggregation benefit** over the single-unit baseline be an increasing, continuous function \$B(\\alpha,D)\$ capturing variance reduction (Eq. V) and feature enrichment across units. The **net utility lift** of the joint DP model vs baseline is

\$\$
\\Delta U(\\alpha,\\varepsilon)
= B(\\alpha,D) - \\mathrm{Penalty}_{\\mathrm{DP}}(\\alpha,\\varepsilon).
\\tag{U}
\$\$

---

# P3 Optimal-Intersection Principle (P3-OIP)

**Assumptions.**

1. \$B(\\alpha,D)\$ is continuous, nondecreasing in \$\\alpha\$, and \$\\lim_{\\alpha\\to 0} B(\\alpha,D)=0\$.
2. \$n(\\alpha)\$ is continuous, nondecreasing in \$\\alpha\$, hence \$n_{\\mathrm{eff}}(\\alpha)\$ in (ESS) is continuous and nondecreasing.
3. \$\\mathrm{Penalty}_{\\mathrm{DP}}(\\alpha,\\varepsilon) = K/[n_{\\mathrm{eff}}(\\alpha)\\varepsilon^p]\$ is continuous and nonincreasing in \$\\alpha\$ (for fixed \$\\varepsilon\$ ).
4. Baseline utility \$U_{\\mathrm{base}}\$ is finite; utilities compare by difference \$\\Delta U\$ in (U).

**Theorem (P3-OIP).**
For any fixed \$D\\ge 2\$ and \$\\varepsilon>0\$, there exists at least one \$\\alpha^\\star \\in [0,1]\$ such that

\$\$
\\Delta U(\\alpha^\\star,\\varepsilon)\\ge 0,
\$\$

and for all \$\\alpha > \\alpha^\\star\$ (within feasibility), \$\\Delta U(\\alpha,\\varepsilon) \\ge 0\$. In words: **there exists an optimal intersection level** at which the joint DP model **matches or exceeds** single-dataset performance, and any higher intersection continues to outperform (until other constraints bind).

**Proof (sketch).**
By Assumptions 1–3, \$B(\\alpha,D)\$ is monotone increasing and \$\\mathrm{Penalty}_{\\mathrm{DP}}(\\alpha,\\varepsilon)\$ is monotone decreasing in \$\\alpha\$. Define \$g(\\alpha) = \\Delta U(\\alpha,\\varepsilon)\$. Then \$g\$ is continuous, nondecreasing in \$\\alpha\$. At \$\\alpha=0\$, \$g(0)=0-\\frac{K}{n_{\\mathrm{eff}}(0)\\varepsilon^p}\\le 0\$. For sufficiently large \$\\alpha\$ (more unique customers and views), \$B(\\alpha,D)\$ can be made \$> \\frac{K}{n_{\\mathrm{eff}}(\\alpha)\\varepsilon^p}\$ because \$B\$ grows while the DP penalty shrinks with \$n_{\\mathrm{eff}}\$. By the Intermediate Value Theorem, there exists \$\\alpha^\\star\$ where \$g(\\alpha^\\star) = 0\$. Monotonicity then implies \$g(\\alpha)\\ge 0\$ for all \$\\alpha \\ge \\alpha^\\star\$. \$\\square\$

**Comparative statics (managerial corollaries).**

* **More units** ( \$D \\uparrow\$ ) → \$B(\\cdot) \\uparrow\$, \$\\alpha^\\star \\downarrow\$.
* **Larger population** ( \$n \\uparrow\$ ) → penalty \$\\downarrow\$, \$\\alpha^\\star \\downarrow\$.
* **Higher correlation** ( \$\\rho(\\alpha) \\uparrow\$ ) → \$n_{\\mathrm{eff}} \\downarrow\$, \$\\alpha^\\star \\uparrow\$ (need more overlap/units to overcome redundancy).
* **Weaker privacy** (larger \$\\varepsilon\$ ) → penalty \$\\downarrow\$, \$\\alpha^\\star \\downarrow\$.
* **Stronger privacy** (smaller \$\\varepsilon\$) → need either more data or more diverse units.

---

# Instantiating \$B(\\alpha,D)\$ from variance and features

A practical form that links to (V) is

\$\$
B(\\alpha,D) \\approx c_1\\left[\\sigma^2 - \\sigma^2\\Big(\\rho(\\alpha)+\\tfrac{1-\\rho(\\alpha)}{D}\\Big)\\right]
+ c_2\\Delta\\Phi(\\alpha),
\$\$

where:

* the first bracket is the **variance drop** when moving from one view to \$D\$ views at overlap \$\\alpha\$;
* \$\\Delta\\Phi(\\alpha)\$ is **feature-enrichment gain** (e.g., information gain or mutual information increase due to added views);
* \$c_1,c_2>0\$ weight how variance and features translate to your utility.

Combine this with (ESS) and (P) to compute the **decision inequality**

\$\$
B(\\alpha,D) \\ge \\frac{K}{n_{\\mathrm{eff}}(\\alpha)\\varepsilon^p}.
\\tag{Decision}
\$\$

---

# Policy layer (FOL) connecting back to privacy law (FCRA example)

Let \$\\mathrm{JoinDP}(\\alpha,\\varepsilon)\$ denote the DP-anonymized joint data at overlap \$\\alpha\$. We encode your governance as:

1. \$\\mathrm{JoinDP}(\\alpha,\\varepsilon) \\rightarrow \\mathrm{DP}_\\varepsilon(x)\$.
2. \$\\mathrm{DP}_\\varepsilon(x) \\rightarrow \\mathrm{PermissibleUse}(x)\$.
3. \$\\mathrm{PermissibleUse}(x) \\wedge \\text{DecisionIneq}(\\alpha,\\varepsilon) \\rightarrow \\text{Deploy}(h_{\\mathrm{dp}})\$.

Where \$\\text{DecisionIneq}\$ is the Boolean evaluation of (Decision). This gives you a **machine-checkable** guardrail: only deploy joint models when both **legal** and **utility-dominant**.

---

# How to use P3-OIP in practice

1. **Estimate \$\\rho(\\alpha)\$ :** Fit unit-level models, compute residual correlations on a common holdout; fit a smooth \$\\rho(\\alpha)\$.
2. **Estimate \$n(\\alpha)\$ :** From customer graphs and overlap plans. Compute \$n_{\\mathrm{eff}}(\\alpha)\$ via (ESS).
3. **Pick \$\\varepsilon\$ :** From your privacy budget.
4. **Calibrate \$K,p\$ :** From your DP mechanism/library or pilot results.
5. **Compute \$B(\\alpha,D)\$ :** From observed variance drops and incremental features.
6. **Solve (Decision):** Find smallest \$\\alpha^\\star\$ such that the inequality holds. Operate at or above \$\\alpha^\\star\$.

---

# Optional: stronger guarantee via Bayes-risk monotonicity

If additional views deliver strictly positive mutual information about \$Y\$ conditional on existing features (for a nondegenerate label), the Bayes risk decreases with added views. Under mild smoothness, the **excess-risk** term dominates the DP penalty as \$n(\\alpha)\$ grows, so P3-OIP still yields an \$\\alpha^\\star\$ even when \$\\rho(\\alpha)\$ is high — provided the union truly adds information.

---

## Foundations underpinning P3-OIP (theory and governance)

The theory rests on three pillars that are integral to the statements and equations above:

1. Ensemble variance reduction (Eq. V): Standard ensemble results show that averaging weakly correlated predictors reduces variance, which underpins why the aggregation benefit \$B(\\alpha,D)\$ grows with \$D\$ and depends on \$\\rho(\\alpha)\$.
2. Differential privacy excess-risk bounds (Eqs. DP and P): Under \$\\varepsilon\$ -DP mechanisms (e.g., objective perturbation, DP-SGD), expected excess risk scales like \$C/(n_{\\mathrm{eff}}(\\alpha)\\,\\varepsilon^p)\$, motivating the penalty term \$\\mathrm{Penalty}_{\\mathrm{DP}}(\\alpha,\\varepsilon)\$ and its role in the decision inequality (Decision).
3. Legal non-identifiability implies permissibility for cross-unit modelling (FCRA example): By DP post-processing, \$\\mathrm{DP}_\\varepsilon(x)\$ yields non-identifiable outputs, which (in FCRA terms) are not consumer reports and thus are permissible to use across units; analogous mappings exist under GDPR/CCPA for anonymized data. This connects the theoretical deployment condition to legal governance.

`;
    
    // Block math
    processedContent = processedContent.replace(/\$\$([\s\S]+?)\$\$/g, function(match, math) {
      try {
        return katex.renderToString(math.trim(), { displayMode: true, throwOnError: false });
      } catch (e) { return match; }
    });
    
    // Inline math
    processedContent = processedContent.replace(/\$([^\$\n]+?)\$/g, function(match, math) {
      try {
        return katex.renderToString(math.trim(), { displayMode: false, throwOnError: false });
      } catch (e) { return match; }
    });
    
    // Render
    document.getElementById('content').innerHTML = marked.parse(processedContent);
    
    // Render mermaid
    mermaid.run({ nodes: document.querySelectorAll('.mermaid') });
  </script>
</body>
</html>